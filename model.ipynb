{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eye-tracker pupil detection model   \n",
    "<hr></hr>\n",
    "<b>Author:</b> Owen Burt  \n",
    "\n",
    "<b>Date created:</b> 2/21/2025\n",
    "\n",
    "<b>Last modified</b> 3/6/2025  \n",
    "\n",
    "<hr></hr>\n",
    "\n",
    "<b>Description:</b>  \n",
    "\n",
    "In this notebook I prepare data and construct a model that predicts the location of a pupil. \n",
    "\n",
    "<b>References:</b>  \n",
    "- Referenced <b>mathworks.com</b> to learn about r-cnn, fast r-cnn, and faster r-cnn.\n",
    "- Referenced and used code from the AI text book 'Applied Machine Learning and AI for Engineers' used in the AI/ML course by <b>Jeff Prosise</b>.\n",
    "- Referenced and used code from <b>Tensorflow</b> to help understand tensorflows framework, use their pre-trained models, and output images with matlibplot.\n",
    "- Referneced paper on Faster Convolutional Neural Networks from Cornell Univeristy called 'Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks' by <b>Shaoqing Ren, et al</b>. I used this article to help me understand the Faster R-CNN.\n",
    "\n",
    "<b>Sources:</b>  \n",
    "- mathworks.com: https://www.mathworks.com/help/vision/ug/getting-started-with-r-cnn-fast-r-cnn-and-faster-r-cnn.html\n",
    "- Tensorflow: https://www.tensorflow.org/tutorials/images/cnn\n",
    "- Cornell Univeristy: https://arxiv.org/abs/1506.01497  \n",
    "\n",
    "\n",
    "<hr></hr>\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os as os\n",
    "import matplotlib as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "\n",
    "I'm starting with a folder containing all the images and their corresponding annotations as well as a csv that contains the file name of the image and the boundries of the pupil. In order to train a model on this data I will need each image (features) to be a 3D array conatinaing pixel data and the boundries (target) to be an array that corresponds to the correct image in the 3D array of images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "df = pd.read_csv(cwd + \"/eye_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>obj_class</th>\n",
       "      <th>x1</th>\n",
       "      <th>y1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WIN_20250205_16_00_55_Pro.jpg</td>\n",
       "      <td>pupil</td>\n",
       "      <td>257</td>\n",
       "      <td>163</td>\n",
       "      <td>276</td>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WIN_20250205_16_00_56_Pro.jpg</td>\n",
       "      <td>pupil</td>\n",
       "      <td>279</td>\n",
       "      <td>128</td>\n",
       "      <td>308</td>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WIN_20250205_16_00_57_Pro.jpg</td>\n",
       "      <td>pupil</td>\n",
       "      <td>313</td>\n",
       "      <td>138</td>\n",
       "      <td>346</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WIN_20250205_16_00_58_Pro.jpg</td>\n",
       "      <td>pupil</td>\n",
       "      <td>278</td>\n",
       "      <td>141</td>\n",
       "      <td>305</td>\n",
       "      <td>172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WIN_20250205_16_00_59_Pro.jpg</td>\n",
       "      <td>pupil</td>\n",
       "      <td>263</td>\n",
       "      <td>150</td>\n",
       "      <td>286</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       file_name obj_class   x1   y1   x2   y2\n",
       "0  WIN_20250205_16_00_55_Pro.jpg     pupil  257  163  276  195\n",
       "1  WIN_20250205_16_00_56_Pro.jpg     pupil  279  128  308  156\n",
       "2  WIN_20250205_16_00_57_Pro.jpg     pupil  313  138  346  170\n",
       "3  WIN_20250205_16_00_58_Pro.jpg     pupil  278  141  305  172\n",
       "4  WIN_20250205_16_00_59_Pro.jpg     pupil  263  150  286  181"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_image(image, x1, y1, x2, y2):\n",
    "    ''' \n",
    "    view_image displays the bounding box on the image.\n",
    "\n",
    "    :param image: a numpy array representing the image.\n",
    "    :param x1: x min coordinates.\n",
    "    :param y1: y min coordinates.\n",
    "    :param x2: x max coordinates.\n",
    "    :param y2: y max coordinates.\n",
    "    '''\n",
    "\n",
    "    image = image.copy()\n",
    "    color = (0, 255, 0)\n",
    "    thickness = 2\n",
    "\n",
    "    cv2.rectangle(image, (x1, y1), (x2, y2), color, thickness)\n",
    "    cv2.imshow('Bounding Box', image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = cwd + '\\\\image_data\\\\' + df['file_name'][0]\n",
    "img = cv2.imread(img_path)\n",
    "x1 = df['x1'][0]\n",
    "y1 = df['y1'][0]\n",
    "x2 = df['x2'][0]\n",
    "y2 = df['y2'][0]\n",
    "view_image(img, x1, y1, x2, y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting data into training, testing, and validation groups.\n",
    "\n",
    "\n",
    "My training data will be 75% of original dataset, testing will be 10%, and validation will be 15%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df['file_name']\n",
    "y = df[['x1', 'y1', 'x2', 'y2']]\n",
    "\n",
    "train_ratio = 0.75\n",
    "validation_ratio = 0.15\n",
    "test_ratio = 0.10\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=1 - train_ratio)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=test_ratio/(test_ratio + validation_ratio)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resetting indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)\n",
    "X_val = X_val.reset_index(drop=True)\n",
    "y_val = y_val.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = cwd + '\\\\image_data\\\\' + X_train[0]\n",
    "img= cv2.imread(img_path)\n",
    "x1 = y_train['x1'][0]\n",
    "y1 = y_train['y1'][0]\n",
    "x2 = y_train['x2'][0]\n",
    "y2 = y_train['y2'][0]\n",
    "view_image(img, x1, y1, x2, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    WIN_20250208_18_45_09_Pro.jpg\n",
       "1    WIN_20250208_18_40_53_Pro.jpg\n",
       "2    WIN_20250205_16_23_19_Pro.jpg\n",
       "3    WIN_20250220_19_58_42_Pro.jpg\n",
       "4    WIN_20250220_19_59_07_Pro.jpg\n",
       "Name: file_name, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>y1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>294</td>\n",
       "      <td>167</td>\n",
       "      <td>309</td>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>274</td>\n",
       "      <td>131</td>\n",
       "      <td>301</td>\n",
       "      <td>161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>271</td>\n",
       "      <td>171</td>\n",
       "      <td>293</td>\n",
       "      <td>202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    x1   y1   x2   y2\n",
       "0  294  167  309  189\n",
       "1  274  131  301  161\n",
       "2  271  171  293  202\n",
       "3    0    0    0    0\n",
       "4    0    0    0    0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting images and boundry data to arrays and normalizing \n",
    "\n",
    "Currently, <b>X_data</b> is a <u>series</u> of file names and <b>y_data</b> is a <u>dataframe</u> containing boundry data. I want the training data to be an <u>array</u> of pixel data and the target data to be an <u>array</u> containing just the boundries with no labels/index. I will create a function that handles both of these tasks. Along with converting data to array format these funcitons will also handle normalizing the data. For the images I do this by dividing the pixel data by 255 since a pixel is comprised of three values ranging from 0 to 255. Dividing by 255 gives me pixel data that ranges from 0 to 1. I treat the boudries similarly and divide x values by the width of my image and y values by the height leaving me with boundry data that ranges form 0 to 1. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# may not need to resize the image if I use the fast r-cnn with tensorflow\n",
    "def resize_image(image):\n",
    "    image = tf.image.resize(image, [512, 512])\n",
    "    return image\n",
    "\n",
    "def images_to_arr(X):\n",
    "    '''\n",
    "    images_to_arr gets image path, loads the image, then converts it to a 3D array. \n",
    "        It also normalizes the pixel values to be between 0 - 1 and converts the image to greyscale.\n",
    "    \n",
    "    :param X: the series of image file names to be converted.\n",
    "    '''\n",
    "    X_data = []\n",
    "    for i in range(len(X)):\n",
    "        img_path = cwd + '\\\\image_data\\\\' + df['file_name'][i]\n",
    "        image = tf.keras.utils.load_img(img_path)\n",
    "        image_as_arr = tf.keras.utils.img_to_array(image)/255\n",
    "        #image_as_arr = resize_image(image_as_arr)/255.0 # resizing\n",
    "        #image_final = tf.image.rgb_to_grayscale(image_as_arr)\n",
    "        X_data.append(image_as_arr)\n",
    "    return np.array(X_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bounds_to_arr(y):\n",
    "    '''\n",
    "    bounds_to_arr converts boundry data from a dataframe to a np array containing just the coordinates. \n",
    "        It also normalizes the boundry values to be between 0 and 1\n",
    "\n",
    "    :param y: the dataframe of boundry data to be converted.\n",
    "    '''\n",
    "    img_width = 640.0\n",
    "    img_hight = 360.0\n",
    "    y_data = []\n",
    "    for i in range(len(y)):\n",
    "        x1 = round(y.iloc[i].iloc[0]/img_width, 3)\n",
    "        y1 = round(y.iloc[i].iloc[1]/img_hight, 3)\n",
    "        x2 = round(y.iloc[i].iloc[2]/img_width, 3)\n",
    "        y2 = round(y.iloc[i].iloc[3]/img_hight, 3)\n",
    "        y_data.append((x1,y1, x2, y2))\n",
    "    return np.array(y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Executing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = images_to_arr(X_train)\n",
    "y_train = bounds_to_arr(y_train)\n",
    "X_test = images_to_arr(X_test)\n",
    "y_test = bounds_to_arr(y_test)\n",
    "X_val = images_to_arr(X_val)\n",
    "y_val = bounds_to_arr(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing data after preprocessing is complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#call view_image func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting and Building a Model  \n",
    "\n",
    "I am beginning by pretty much copying a model from the 'Applied Machine Learning and AI for Engineers' text book to get a benchmark of where the data is at and what my model might need. The originial code can be found in the text book on page 254. I made one edit to the code and changed the loss funciton to mean squared error since my output layer is of size 4.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running a Simple Convolutional Neural Network for Experimentation  \n",
    "\n",
    "- the type of model is important and I should read more about this.  \n",
    "- Conv2D applies a convolution (2-dimentional for images since images are 2D) to the input image. \n",
    "The parameter 64 refers to the number of filters applied in the covolution.\n",
    "a filter is a stack of kernels. In this case each filter has 3 kernels since there are three channels specified in the input_shape param (image are usually RGB hence 3 channels. one for red one for green one for blue)\n",
    "The second param (3, 3) specifies the shape of the kerel applied.\n",
    "I should really learn more about activation functions relu or rectified linear unifying is common but this should be selected based on your data\n",
    "imput shape refers to the size of the image being passed into the layer. In this case I resized all of my images to be 512 pixels wide and 512 pixels high. Each image is not RGB so maybe I should change the channel.\n",
    "- Pooling is done to decrease compute time and helps to control overfitting by reducing the feature map output by the previous layer. \n",
    "Here were using maxpooling2d with shape 2x2 I am pretty sure this slides a 2x2 matrix across the featuremap reducing that 2x2 portion of the \n",
    "image down to a 1x1 block containing the maximum value in the 2x2 matrix.\n",
    "- Fully connected layers require 1D imputs. A flattening layer is added to convert 2D or 3D tensor (tensor is can be thought of as a n-dimentional matrix in this case 2x2 matrix) into a 1D vector\n",
    "- The dense layers convert the flattened feature maps to probabilities. the first param is how many output units youwant\n",
    "In this case my final dense layer has 4 units since I am predicting 4 points for boundries. \n",
    "activation matters and I should look more into this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "________________________________________________________________________________\n",
      " Layer (type)                       Output Shape                    Param #     \n",
      "================================================================================\n",
      " conv2d_8 (Conv2D)                  (None, 358, 638, 24)            672         \n",
      "                                                                                \n",
      " max_pooling2d_8 (MaxPooling2D)     (None, 179, 319, 24)            0           \n",
      "                                                                                \n",
      " conv2d_9 (Conv2D)                  (None, 177, 317, 32)            6944        \n",
      "                                                                                \n",
      " max_pooling2d_9 (MaxPooling2D)     (None, 88, 158, 32)             0           \n",
      "                                                                                \n",
      " conv2d_10 (Conv2D)                 (None, 86, 156, 64)             18496       \n",
      "                                                                                \n",
      " max_pooling2d_10 (MaxPooling2D)    (None, 43, 78, 64)              0           \n",
      "                                                                                \n",
      " conv2d_11 (Conv2D)                 (None, 40, 75, 128)             131200      \n",
      "                                                                                \n",
      " max_pooling2d_11 (MaxPooling2D)    (None, 20, 37, 128)             0           \n",
      "                                                                                \n",
      " flatten_2 (Flatten)                (None, 94720)                   0           \n",
      "                                                                                \n",
      " dense_4 (Dense)                    (None, 512)                     48497152    \n",
      "                                                                                \n",
      " dense_5 (Dense)                    (None, 4)                       2052        \n",
      "                                                                                \n",
      "================================================================================\n",
      "Total params: 48,656,516\n",
      "Trainable params: 48,656,516\n",
      "Non-trainable params: 0\n",
      "________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#one edit I made was to change the loss funciton to mean squared error.\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.losses import Huber\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "loss = Huber(delta=1.0)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(24, (3, 3), activation='relu', input_shape=(360, 640, 3)))\n",
    "model.add(MaxPooling2D(2, 2)) \n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(2, 2))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(2, 2))\n",
    "model.add(Conv2D(128, (4, 4), activation='relu'))\n",
    "model.add(MaxPooling2D(2, 2)) \n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(4, activation='linear'))\n",
    "model.compile(optimizer=SGD(learning_rate=0.01, momentum=0.9), loss=loss,\n",
    "             metrics=['accuracy'])\n",
    "model.summary(line_length=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_accuracy_metrics(hist):\n",
    "    ''' \n",
    "    view_accuracy_metrics displays a graph showing training and validation accuracy.\n",
    "\n",
    "    :param hist: object containing info about training process.\n",
    "    '''\n",
    "    sns.set()\n",
    "    acc = hist.history['accuracy']\n",
    "    val_acc = hist.history['val_accuracy']\n",
    "    epochs = range(1, len(acc) + 1)\n",
    "    plt.plot(epochs, acc, '-', label='Training Accuracy')\n",
    "    plt.plot(epochs, val_acc, ':', label='Validation Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(X_train, y_train,\n",
    "                validation_data=(X_test, y_test),\n",
    "                batch_size=100, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_accuracy_metrics(hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copying VGG16 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_feat_map(model, data):\n",
    "    ''' \n",
    "    show_feat_map displays the feature map (image) after each filter is applied. \n",
    "\n",
    "    :param result: takes in the 4D tensor output from MaxPooling2D layer with format (batch_size, rows, cols, channels). \n",
    "    '''\n",
    "\n",
    "    model.build()\n",
    "    model.summary()\n",
    "    result = model.predict(data)\n",
    "    for i in range(64):\n",
    "        feature_img = result[0, :, :, i]\n",
    "        ax = plt.subplot(8, 8, i+1)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        plt.imshow(feature_img, cmap='grey')\n",
    "    plt.show\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Block 1\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), padding='same', activation='relu', input_shape=(360, 640, 3)))\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_feat_map(model, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Block 2\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Block 3\n",
    "model.add(Conv2D(256, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "model.add(Conv2D(256, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_feat_map(model, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Block 4\n",
    "model.add(Conv2D(512, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "model.add(Conv2D(512, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "model.add(Conv2D(512, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Block 5\n",
    "model.add(Conv2D(512, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "model.add(Conv2D(512, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "model.add(Conv2D(512, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_feat_map(model, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(4, activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), loss='smooth_l1', metrics='mae')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(X_train, y_train,\n",
    "                validation_data=(X_test, y_test),\n",
    "                batch_size=100, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_accuracy_metrics(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eyetracker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
